---
version: "3.8"

x-airflow-common:
  &airflow-common
  build:
    context: .
    dockerfile: ./Dockerfile.airflow
  environment:
    &airflow-common-env
    AIRFLOW__CORE__EXECUTOR: LocalExecutor
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: mssql+pyodbc://${AIRFLOW_DB_USER}:${AIRFLOW_DB_PASSWORD}@${AIRFLOW_DB_SERVER}/${AIRFLOW_DB_NAME}?driver=ODBC+Driver+18+for+SQL+Server&TrustServerCertificate=yes
    AIRFLOW__CORE__FERNET_KEY: ''
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
    AIRFLOW__CORE__LOAD_EXAMPLES: 'false'
    AIRFLOW__API__AUTH_BACKENDS: 'airflow.api.auth.backend.basic_auth,airflow.api.auth.backend.session'
    # Use simple http server on scheduler for health checks
    # See https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/logging-monitoring/check-health.html#scheduler-health-check-server
    # yamllint enable rule:line-length
    AIRFLOW__SCHEDULER__ENABLE_HEALTH_CHECK: 'true'
    # WARNING: Use _PIP_ADDITIONAL_REQUIREMENTS option ONLY for a quick checks
    # for other purpose (development, test and especially production usage) extend './airflow/Dockerfile'.
    _PIP_ADDITIONAL_REQUIREMENTS: ${_PIP_ADDITIONAL_REQUIREMENTS:-}
    AIRFLOW_UID: ${AIRFLOW_UID}
    USER: ${USER}
    AIRFLOW__LOGGING__REMOTE_LOGGING: 'true'
    AIRFLOW__LOGGING__LOGGING_CONFIG_CLASS: 'log_config.DEFAULT_LOGGING_CONFIG'
    AIRFLOW__LOGGING__REMOTE_BASE_LOG_FOLDER: ${AIRFLOW__LOGGING__REMOTE_BASE_LOG_FOLDER}
    AIRFLOW__LOGGING__REMOTE_LOG_CONN_ID: 'azure_blob'
    AIRFLOW_CONN_AZURE_BLOB: |
     {
        "conn_type": "wasb",
        "password":"${AZURE_BLOB_PASSWORD}",
        "host": "${AZURE_BLOB_HOST}",
        "extra": {
          "connection_string": "${AZURE_BLOB_CONNECTION_STRING}"
        }
      }

  volumes:
    - ${AIRFLOW_PROJ_DIR:-.}/dags:/opt/airflow/dags
    - ${AIRFLOW_PROJ_DIR:-.}/configs:/opt/airflow/plugins
    # - ${AIRFLOW_PROJ_DIR:-.}/config:/opt/airflow/config
    - airflow-logs-volume:/opt/airflow/logs
    - ${DOCKER_SOCKET_PATH:-//var/run/docker.sock}:/var/run/docker.sock
  user: "${AIRFLOW_UID:-50000}:0"
  depends_on:
    &airflow-common-depends-on
    sql-server:
      condition: service_healthy
    build-hello-image:
      condition: service_completed_successfully
    build-goodbye-image:
      condition: service_completed_successfully

services:
  build-base-image:
    build:
      context: .
      dockerfile: ./Dockerfile.base
    image: app-base-image:latest
    entrypoint: ["sh", "-c", "echo 'app-base-image:latest' was successfully built!"]

  build-hello-image:
    build:
      context: .
      dockerfile: ./Dockerfile.hello
    image: hello:latest
    entrypoint: ["sh", "-c", "echo 'hello:latest' was successfully built!"]
    depends_on:
      build-base-image:
        condition: service_completed_successfully

  build-goodbye-image:
    build:
      context: .
      dockerfile: ./Dockerfile.goodbye
    image: goodbye:latest
    entrypoint: ["sh", "-c", "echo 'goodbye:latest' was successfully built!"]
    depends_on:
      build-base-image:
        condition: service_completed_successfully

  sql-server:
    image: mcr.microsoft.com/mssql/server:2022-latest
    environment:
      ACCEPT_EULA: Y
      MSSQL_SA_PASSWORD: ${MSSQL_SA_PASSWORD}
    ports:
      - 1433:1433
    healthcheck:
      test: /opt/mssql-tools/bin/sqlcmd -S localhost -U ${MSSQL_SA_USER} -P "$${MSSQL_SA_PASSWORD}" -Q "SELECT 1" -b -o /dev/null
      interval: 10s
      timeout: 3s
      retries: 10
      start_period: 10s
    restart: always
    volumes:
      - sql-volume:/var/opt/mssql
  # This service waits for the sql-server to start and then it executes SQL scripts
  sql-setup:
    image: mcr.microsoft.com/mssql/server:2022-latest
    environment:
      MSSQL_HOST: sql-server
      MSSQL_SA_USER: ${MSSQL_SA_USER}
      MSSQL_SA_PASSWORD: ${MSSQL_SA_PASSWORD}
      AIRFLOW_DB_NAME: ${AIRFLOW_DB_NAME}
    volumes:
      - ./sql-scripts:/sql-scripts
    working_dir: /sql-scripts
    depends_on:
      - sql-server
    entrypoint: "./init-db.sh"

  airflow-webserver:
    <<: *airflow-common
    command: webserver
    ports:
      - "8080:8080"
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  airflow-scheduler:
    <<: *airflow-common
    command: scheduler
    healthcheck:
      test: ["CMD", "curl", "--fail", "http://localhost:8974/health"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  airflow-triggerer:
    <<: *airflow-common
    command: triggerer
    healthcheck:
      test: ["CMD-SHELL", 'airflow jobs check --job-type TriggererJob --hostname "$${HOSTNAME}"']
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  airflow-init:
    <<: *airflow-common
    entrypoint: /bin/bash
    command:
      - -c
      - |
        mkdir -p /sources/dags
        mkdir -p /sources/configs
        chown -R "${AIRFLOW_UID}:0" /sources/dags
        chown -R "${AIRFLOW_UID}:0" /sources/configs
        exec /entrypoint airflow version
    environment:
      <<: *airflow-common-env
      _AIRFLOW_DB_UPGRADE: 'true'
      _AIRFLOW_WWW_USER_CREATE: 'true'
      _AIRFLOW_WWW_USER_USERNAME: ${_AIRFLOW_WWW_USER_USERNAME:-airflow}
      _AIRFLOW_WWW_USER_PASSWORD: ${_AIRFLOW_WWW_USER_PASSWORD:-airflow}
      _PIP_ADDITIONAL_REQUIREMENTS: ''
    user: "0:0"
    volumes:
      - ${AIRFLOW_PROJ_DIR:-.}:/sources

volumes:
  sql-volume:
  airflow-logs-volume:
